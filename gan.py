import tensorflow as tf
import numpy as np 
import time
import matplotlib.pyplot as plt

(train_images, train_labels),(test_images, test_labels)=tf.keras.datasets.mnist.load_data()

plt.imshow(train_images[0])

#reshaping the images before normalization 
train_images = train_images.reshape(train_images.shape[0],28,28,1) 

#for keeping the data between [-1,1] called normalization
train_images = (train_images-127.5)/127.5 

#splitting the dataset into smaller fragments with buffer size 
#and then shuffling it to avoid same numbers grouping together
BUFFER_SIZE = train_images.shape[0]
BATCH_SIZE = 100
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

#DISCRIMINATOR MODEL
#outputs a number between 0 and 1, giving the probability that 
#how accurately the model can identify whether it is a training
#dataset or generated by the generator
def make_discriminator_model():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Conv2D(7,(3,3),padding='same',input_shape=[28,28,1]))
  model.add(tf.keras.layers.Flatten()) #flattening the layers before putting it into the dense layer
  model.add(tf.keras.layers.LeakyReLU()) #adding non-linearity
  model.add(tf.keras.layers.Dense(50,activation="relu"))
  model.add(tf.keras.layers.Dense(1))
  return model

model_discriminator = make_discriminator_model()
model_discriminator(np.random.rand(1,28,28,1).astype("float"))

#Adding Optimizers
discriminator_optimizer = tf.optimizers.Adam(1e-3)

#the discriminator is rewarded if it sees the image
#generated by the generator as fake
def get_discriminator_loss(real_predictions, fake_predictions) :
  real_predictions = tf.sigmoid(real_predictions) 
  fake_predictions = tf.sigmoid(fake_predictions)
#real predictions come from the real images and the fake predictions 
#come from the generator
  real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_predictions),real_predictions)
  fake_loss = tf.losses.binary_crossentropy(tf.zeros_like(fake_predictions),fake_predictions)
  return fake_loss+real_loss

#GENERATOR MODEL
#takes an input of random string of numbers 
#and output something that looks like MNIST digit
def make_generator_model() :
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(7*7*256,input_shape = (100,))) #256 filters
  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Reshape((7,7,256)))
  model.add(tf.keras.layers.Conv2DTranspose(128,(3,3),padding="same"))
  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2DTranspose(64,(3,3),strides=(2,2),padding="same"))
  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2DTranspose(1,(3,3),strides=(2,2),padding="same"))
  return model

generator_optimizer = tf.optimizers.Adam(1e-4)

#here the generator will be rewarded if the discriminator
#sees the fake image as real
def get_generator_loss(fake_predictions) :
  fake_predictions = tf.sigmoid(fake_predictions)
  fake_loss = tf.losses.binary_crossentropy(tf.ones_like(fake_predictions),fake_predictions)
  return fake_loss

#here the generator will be rewarded if the discriminator
#sees the fake image as real
def get_generator_loss(fake_predictions) :
  fake_predictions = tf.sigmoid(fake_predictions)
  fake_loss = tf.losses.binary_crossentropy(tf.ones_like(fake_predictions),fake_predictions)
  return fake_loss

  #we will create some fake noise for the generator
#which will produce some fake images
#and this will be used to produce fake output and real output(meaning
#the chance of this noise being a real or fake image)
def train_step(images):
  fake_image_noise = np.random.randn(BATCH_SIZE,100).astype("float32")
  with tf.GradientTape() as gen_tape,tf.GradientTape() as disc_tape:
    generated_images = generator(fake_image_noise)
    real_output = model_discriminator(images)
    fake_output = model_discriminator(generated_images)

    gen_loss = get_generator_loss(fake_output)
    disc_loss = get_discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss,generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss,model_discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator,generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,model_discriminator.trainable_variables))

    print("generator loss : ", np.mean(gen_loss))
    print("discriminator loss : ",np.mean(disc_loss))

train(train_dataset, 2)